#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
SYNOPSIS

tv_grab_nl_py is a python script that trawls tvgids.nl for TV
programming information and outputs it in XMLTV-formatted output (see
http://membled.com/work/apps/xmltv). Users of MythTV
(http://www.mythtv.org) will appreciate the output generated by this
grabber, because it fills the category fields, i.e. colors in the EPG,
and has logos for most channels automagically available. Check the
website below for screenshots.  The newest version of this script can be
found here:

     http://code.google.com/p/tvgrabnlpy/

USAGE

Check the web site above and/or run script with --help and start from there

REQUIREMENTS

* Python 2.6 or 2.7
* Connection with the Internet

QUESTIONS

Questions (and patches) are welcome at:
http://www.pwdebruin.net/mailman/listinfo/tv_grab_nl_py_pwdebruin.net
http://code.google.com/p/tvgrabnlpy/issues/list

UPGRADE NOTES

If you were using tv_grab_nl from the XMLTV bundle then enable the
compat flag or use the --compat command-line option.  Otherwise, the
xmltvid's are wrong and you will not see any new data in MythTV.

HISTORY

tv_grab_nl_py used to be called tv_grab_nl_pdb, created by Paul de Bruin
and first released on 2003/07/09. At the same time the code base switched
from using CVS to SVN at Google Code, and as a result the version numbering
scheme has changed. The lastest official release of tv_grab_nl_pdb is 0.48.
The first official release of tv_grab_nl_py is 6. In 2012, The codebase
moved to Git, and the version number was changed once more. The latest
subversion release of tv_grab_nl_py is r109. The first Git release of
tv_grab_nl_py is 2012-04-11 12:03.

CONTRIBUTORS

Main author: Paul de Bruin (paul at pwdebruin dot net)
Current maintainer: Freek Dijkstra (software at macfreek dot nl)

Michel van der Laan made available his extensive collection of
high-quality logos that is used by this script.

Several other people have provided feedback and patches:
Huub Bouma, Michael Heus, Udo van den Heuvel, Han Holl, Hugo van der Kooij, 
Roy van der Kuil, Ian Mcdonald, Dennis van Onselen, Remco Rotteveel, Paul 
Sijben, Willem Vermin, Michel Veerman, Sietse Visser, Mark Wormgoor.

LICENSE

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
""" 

# Python 3 compatibility
from __future__ import unicode_literals
# from __future__ import print_function

# Modules we need
import re, getopt, sys, codecs
import time, random
import os, os.path, pickle
try:
    import urllib.request as urllib
except ImportError:
    import urllib2 as urllib
try:
    from html.entities import name2codepoint
except ImportError:
    from htmlentitydefs import name2codepoint
from threading import Thread
from xml.sax import saxutils
import io
import json
try:
    unichr(42)
except NameError:
    unichr = chr    # Python 3

# Extra check for the datetime module 
try:
    import datetime
except ImportError:
    sys.stderr.write('This script needs the datetime module that was introduced in Python version 2.3.\n')
    sys.stderr.write('You are running:\n')
    sys.stderr.write('%s\n' % sys.version)
    raise

import sys,codecs,locale


VERSION = "2012-04-12"
# VERSION += "-experimental"


# XXX: fix to prevent crashes in Snow Leopard [Robert Klep]
if sys.platform == 'darwin' and sys.version_info[:3] == (2, 6, 1):
    try:
        urllib.urlopen('http://localhost.localdomain')
    except Exception:
        pass


# globals

tvgids = 'http://www.tvgids.nl/'
channels_zoeken = tvgids + 'json/lists/channels.php'
uitgebreid_zoeken = tvgids + 'json/lists/programs.php'

# how many seconds to wait before we timeout on a 
# url fetch, 10 seconds seems reasonable
global_timeout = 10

# Wait a random number of seconds between each page fetch.
# We want to be nice and not hammer tvgids.nl (these are the 
# friendly people that provide our data...).
# Also, it appears tvgids.nl throttles its output.
# So there, there is not point in lowering these numbers, if you 
# are in a hurry, use the (default) fast mode.
nice_time = [1, 2]

# Maximum length in minutes of gaps/overlaps between programs to correct
max_overlap = 10

# Strategy to use for correcting overlapping prgramming:
# 'average' = use average of stop and start of next program
# 'stop'    = keep stop time of current program and adjust start time of next program accordingly
# 'start'   = keep start time of next program and adjust stop of current program accordingly
# 'none'    = do not use any strategy and see what happens
overlap_strategy = 'average'

# Experimental strategy for clumping overlapping programming, all programs that overlap more
# than max_overlap minutes, but less than the length of the shortest program are clumped 
# together. Highly experimental and disabled for now.
do_clump = False

# Create a category translation dictionary
# Look in mythtv/themes/blue/ui.xml for all category names
# The keys are the categories used by tvgids.nl (lowercase please)
cattrans = { 'amusement'        : 'Talk',
             'animatie'         : 'Animated',
             'comedy'           : 'Comedy',
             'documentaire'     : 'Documentary',
             'educatief'        : 'Educational',
             'erotiek'          : 'Adult',
             'film'             : 'Film',
             'muziek'           : 'Art/Music',
             'informatief'      : 'Educational',
             'jeugd'            : 'Children',
             'kunst/cultuur'    : 'Arts/Culture',
             'misdaad'          : 'Crime/Mystery',
             'muziek'           : 'Music',
             'natuur'           : 'Science/Nature',
             'nieuws/actualiteiten' : 'News',
             'overige'          : 'Unknown',
             'religieus'        : 'Religion',
             'serie/soap'       : 'Drama',
             'sport'            : 'Sports',
             'theater'          : 'Arts/Culture',
             'wetenschap'       : 'Science/Nature'}

# Create a role translation dictionary for the xmltv credits part
# The keys are the roles used by tvgids.nl (lowercase please)
roletrans = {'regisseur'         : 'director',
             'regie'             : 'director',
             'acteurs'           : 'actor',
             'presentatie'       : 'presenter',
             'scenario'          : 'writer'}

# We have two sources of logos, the first provides the nice ones, but is not 
# complete. We use the tvgids logos to fill the missing bits.
logo_provider = [ 'http://graphics.tudelft.nl/~paul/logos/gif/64x64/',
                  'http://static.tvgids.nl/gfx/zenders/' ]

logo_names = { 
            1 : [0, 'ned1'],
            2 : [0, 'ned2'],
            3 : [0, 'ned3'],
            4 : [0, 'rtl4'],
            5 : [0, 'een'],
            6 : [0, 'canvas_color'],
            7 : [0, 'bbc1'],
            8 : [0, 'bbc2'],
            9 : [0,'ard'],
            10 : [0,'zdf'],
            11 : [1, 'rtl'],
            12 : [0, 'wdr'],
            13 : [1, 'ndr'],
            14 : [1, 'srsudwest'],
            15 : [1, 'rtbf1'],
            16 : [1, 'rtbf2'],
            17 : [0, 'tv5'],
            18 : [0, 'ngc'],
            19 : [1, 'eurosport'],
            20 : [1, 'tcm'],
            21 : [1, 'cartoonnetwork'],
            24 : [0, 'canal+red'],
            25 : [0, 'mtv-color'],
            26 : [0, 'cnn'],
            27 : [0, 'rai'],
            28 : [1, 'sat1'],
            29 : [0, 'discover-spacey'],
            31 : [0, 'rtl5'],
            32 : [1, 'trt'],
            34 : [0, 'veronica'],
            35 : [0, 'tmf'],
            36 : [0, 'sbs6'],
            37 : [0, 'net5'],
            38 : [1, 'arte'],
            39 : [0, 'canal+blue'],
            40 : [0, 'at5'],
            46 : [0, 'rtl7'],
            49 : [1, 'vtm'],
            50 : [1, '3sat'],
            58 : [1, 'pro7'],
            59 : [1, 'kanaal2'],
            60 : [1, 'vt4'],
            65 : [0, 'animal-planet'],
            73 : [1, 'mezzo'],
            86 : [0, 'bbc-world'],
            87 : [1, 'tve'],
            89 : [1, 'nick'],
            90 : [1, 'bvn'],
            91 : [0, 'comedy_central'],
            92 : [0, 'rtl8'],
            99 : [1, 'sport1_1'],
            100 : [0, 'rtvu'],
            101 : [0, 'tvwest'],
            102 : [0, 'tvrijnmond'],
            103 : [1, 'tvnoordholland'],
            104 : [1, 'bbcprime'],
            105 : [1, 'spiceplatinum'],
            107 : [0, 'canal+yellow'],
            108 : [0, 'tvnoord'],
            109 : [0, 'omropfryslan'],
            114 : [0, 'omroepbrabant']}

# A selection of user agents we will impersonate, in an attempt to be less
# conspicuous to the tvgids.nl police.

user_agents = [ 'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)',
       'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.9) Gecko/20071025 Firefox/2.0.0.9',
       'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)',
       'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.0.7) Gecko/20060909 Firefox/1.5.0.7',
       'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0)',
       'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.9) Gecko/20071105 Firefox/2.0.0.9',
       'Mozilla/5.0 (Macintosh; U; Intel Mac OS X; en-US; rv:1.8.1.9) Gecko/20071025 Firefox/2.0.0.9',
       'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.1.8) Gecko/20071022 Ubuntu/7.10 (gutsy) Firefox/2.0.0.8'
       ]


def log(message, quiet=False):
    # Prints a warning to stderr.
    # Note: The function encodes all ouput to utf-8. This may be wrong.
    # TODO: use sys.stdout.encoding, locale.getpreferredencoding(), sys.getfilesystemencoding(), and/or 
    #       os.environ["PYTHONIOENCODING"] to determine the correct encoding.
    # TODO: use logging module
    if not quiet:
        sys.stderr.write(message.encode("utf-8"))


# Work in progress, the idea is to cache program categories and
# descriptions to eliminate a lot of page fetches from tvgids.nl
# for programs that do not have interesting/changing descriptions

class ProgramCache:
    """
    A cache to hold program name and category info.
    TVgids stores the detail for each program on a separate URL with an
    (apparently unique) ID. This cache stores the fetched info with the ID.
    New fetches will use the cached info instead of doing an (expensive)
    page fetch.
    """
    def __init__(self, filename=None):
        """
        Create a new ProgramCache object, optionally from file 
        """

        # where we store our info
        self.filename  = filename

        if filename == None:
            self.pdict = {}
        else:
            if os.path.isfile(filename):
                self.load(filename)
            else:
                self.pdict = {}


    def load(self, filename):
        """
        Loads a pickled cache dict from file
        """
        try:
            self.pdict = pickle.load(open(filename,'r'))
        except Exception:
            log('Error loading cache file: %s (possibly corrupt)' % filename)
            self.clear()

    def dump(self, filename):
        """
        Dumps a pickled cache, and makes sure it is valid
        """
        if os.access(filename, os.F_OK):
            try:
                os.remove(filename)
            except Exception:
                log('Cannot remove %s, check permissions' % filename)
        tmpfile = open(filename+'.tmp', 'w')
        pickle.dump(self.pdict, tmpfile)
        try:
            tmpfile.close()
        except IOError:
            pass
        os.rename(filename+'.tmp', filename)

    
    def query(self, program_id):
        """
        Updates/gets/whatever.
        """

        try:
            return self.pdict[program_id]
        except LookupError:
            return None

    def add(self, program):
        """
        Adds a program
        """
        self.pdict[program['ID']] = program

    def clear(self):
        """
        Clears the cache (i.e. empties it)
        """
        self.pdict = {}

    def clean(self):
        """
        Removes all cached programming before today.
        Also removes erroneously cached programming.
        """
        dnow = datetime.date.today()
        for key in self.pdict.keys():
            try:
                p = self.pdict[key]
                if 'stop-time' not in p or 'name' not in p or \
                        self.pdict[key]['stop-time'].date() < dnow or \
                        type(p['name']) != unicode or \
                        self.pdict[key]['name'].lower() == 'onbekend':
                    del self.pdict[key]
            except LookupError:
                pass

class AmsterdamTimeZone(datetime.tzinfo):
    """Timezone information for Amsterdam"""
    def __init__(self):
        # calculate for the current year:
        year = datetime.date.today().year
        d = datetime.datetime(year, 4, 1, 2, 0)  # Starts last Sunday in March 02:00:00
        self.dston = d - datetime.timedelta(days=d.weekday() + 1)
        d = datetime.datetime(year, 11, 1, 2, 0) # Ends last Sunday in October 02:00:00
        self.dstoff = d - datetime.timedelta(days=d.weekday() + 1)
    def utcoffset(self, dt):
        return datetime.timedelta(hours=1) + self.dst(dt)
    def dst(self, dt):
        if self.dston <=  dt.replace(tzinfo=None) < self.dstoff:
            return datetime.timedelta(hours=1)
        else:
            return datetime.timedelta(0)
class UTCTimeZone(datetime.tzinfo):
    """UTZ Timezone"""
    def utcoffset(self, dt):
        return datetime.timedelta(0)
    def dst(self, dt):
        return datetime.timedelta(0)

CET_CEST = AmsterdamTimeZone()
UTC  = UTCTimeZone()


def usage():
    print('tv_grab_nl_py: A grabber that grabs tvguide data from tvgids.nl\n')
    print('and stores it in XMLTV-combatible format.\n')
    print('Usage:')
    print('--help, -h    = print this info')
    print('--configure   = create configfile (overwrites existing file)')
    print('--config-file = name of the configuration file (default = ~/.xmltv/tv_grab_py.conf')
    print('--capabilities = xmltv required option')
    print('--desc-length = maximum allowed length of programme descriptions in bytes.')
    print('--description = prints a short description of the grabber')
    print('--output      = file where to put the output')
    print('--offset      = first day to grab (0=today)')
    print('--days        = # number of days to grab')
    print('--preferredmethod = returns the preferred method to be called')
    print('--fast        = do not grab descriptions of programming')
    print('--slow        = grab descriptions of programming')
    print('--quiet       = suppress all output')
    print('--compat      = append tvgids.nl to the xmltv id (use this if you were using tv_grab_nl)')
    print('--logos 0/1   = insert urls to channel icons (mythfilldatabase will then use these)')
    print('--nocattrans  = do not translate the grabbed genres into MythTV-genres')
    print('--cache       = cache descriptions and use the file to store')
    print('--clean_cache = clean the cache file before fetching')
    print('--clear_cache = empties the cache file before fetching data')
    print('--slowdays    = grab slowdays initial days and the rest in fast mode')
    print('--max_overlap = maximum length of overlap between programming to correct [minutes]')
    print('--overlap_strategy = what strategy to use to correct overlaps (check top of source code)')
    print('--utc         = generate all data in UTC time (use with timezone "auto" in mythtv)')


# Removes HTML or XML character references and entities from a text string.
# source: http://effbot.org/zone/re-sub.htm#unescape-html
#
# @param text The HTML (or XML) source text.
# @return The plain text, as a Unicode string

def unescape(text):
    def fixup(m):
        text = m.group(0)
        if text[:2] == "&#":
            # character reference
            try:
                if text[:3] == "&#x":
                    return unichr(int(text[3:-1], 16))
                else:
                    return unichr(int(text[2:-1]))
            except ValueError:
                pass
        else:
            # named entity
            try:
                text = unichr(name2codepoint[text[1:-1]])
            except KeyError:
                pass
        return text # leave as is
    return re.sub("&#?\w+;", fixup, text)



def filter_line(s):
    """
    Escape XML encoded stuff and remove newlines and tabs in strings (adapted from tv_grab_be)
    """

    # convert escapse HTML entities to their Unicode equivalent
    s = unescape(s)

    # Ik vermoed dat de volgende drie regels overbodig zijn, maar ze doen
    # niet veel kwaad -- Han Holl
    s = re.sub('[\r\n\t]', '', s)
    s = re.sub('(<.*?>)', '', s)

    return s.strip()

def xmlescape(s):
    """Escape <, > and & characters for use in XML"""
    return saxutils.escape(s)

def format_timezone(td, use_utc):
    """
    Given a datetime object, returns a string in XMLTV format
    """
    if use_utc:
        td = td.astimezone(UTC)
    return td.strftime('%Y%m%d%H%M%S %z')

def find_html_encoding(httphead, htmlhead):
    # look for the text '<meta http-equiv="Content-Type" content="application/xhtml+xml; charset=UTF-8" />'
    # in the first 600 bytes of the HTTP page
    m = re.search(r'<meta[^>]+\bcharset=["\']?([A-Za-z0-9\-]+)\b', htmlhead[:512].decode('ascii', 'ignore'))
    if m:
        return m.group(1)
    # Find a HTTP header: Content-Type: text/html; charset=UTF-8
    m = re.search(r'\bcharset=([A-Za-z0-9\-]+)\b', httphead.info().getheader('Content-Type'))
    if m:
        return m.group(1)
    return 'iso-8859-1' # the default HTTP encoding.

def get_page_internal(url, quiet=0):
    """
    Retrieves the url and returns a string with the contents.
    Optionally, returns None if processing takes longer than
    the specified number of timeout seconds.
    """
    txtdata = None
    txtheaders = {'Keep-Alive' : '300',
                  'User-Agent' : user_agents[random.randint(0, len(user_agents)-1)] }
    try:
        #fp = urllib.urlopen(url)
        rurl = urllib.Request(url, txtdata, txtheaders)
        fp = urllib.urlopen(rurl)
        bytes = fp.read()
        encoding = "default encoding"
        page = None
        try:
            encoding = find_html_encoding(fp, bytes)
            # log ('parse %s as %s' % (url, encoding))
            page = bytes.decode(encoding, 'strict')
        except Exception:
            log('Cannot decode url %s as %s\n' % (url, encoding), quiet)
            page = bytes.decode('Windows-1252', 'ignore') # At least gets it somewhat correct
        return page
    except (urllib.URLError) as e:
        log('Cannot open url %s: %s\n' % (url, e.reason), quiet)
        return None
    except (urllib.HTTPError) as e:
        log('Cannot parse url %s: code=%s\n' % (url, e.code), quiet)
        return None

class FetchURL(Thread):
    """
    A simple thread to fetch a url with a timeout
    """
    def __init__ (self, url, quiet=0):
        Thread.__init__(self)
        self.quiet = quiet
        self.url = url
        self.result = None

    def run(self):
        self.result = get_page_internal(self.url, self.quiet)

def get_page(url, quiet=0):
    """
    Wrapper around get_page_internal to catch the
    timeout exception
    """
    try: 
        fu = FetchURL(url, quiet)
        fu.start()
        fu.join(global_timeout)
        return fu.result
    except Exception:
        log('get_page timed out on (>%s s): %s\n' % (global_timeout, url), quiet)
        return None

def get_channels(file, quiet=0):
    """
    Get a list of all available channels and store these
    in a file.
    """

    # download the json feed
    total = get_page(channels_zoeken, quiet)
    if total == None:
        log("Don't write configuration file\n")
        return 69  # EX_UNAVAILABLE
    channel_list = json.loads(total)
        

    # convert to a map, so we can sort it..
    channels = {}

    # and create a file with the channels
    f = open(file,'w')
    f.write("# encoding: utf-8\n")
    for channel in channel_list:
        # the json data has the channel names in XML entities.
        name = unescape(channel['name'])
        chid = int(channel['id'])
        regel = "%s %s\n" % (chid, name)
        f.write(regel.encode('utf-8'))
    f.close()

def get_channel_all_days(channel, start_day, days, quiet=0):
    """
    Get all available days of programming for channel number

    The output is a list of programming in order where each row
    contains a dictionary with program information.
    """
    
    now = datetime.datetime.now()
    
    programs = []
    
    retime = re.compile(r'(\d\d\d\d)-(\d+)-(\d+) (\d+):(\d+)(?::\d+)')
    
    def match_to_date(match, time, program):
        if match:
            return datetime.datetime(int(match.group(1)),int(match.group(2)),\
                    int(match.group(3)),int(match.group(4)),int(match.group(5)),
                    tzinfo=CET_CEST)
        else:
            log("Can not determine %s for %s" % (time,program))
            return None


    # Tvgids shows programs per channel per day, so we loop over the number of days
    # we are required to grab
    for offset in range(start_day, start_day+days):
    
        channel_url = uitgebreid_zoeken + '?channels=%s&day=%s' % (channel, offset)

        if offset > start_day:
            time.sleep(random.randint(nice_time[0], nice_time[1]))
        # get the raw programming for the day
        strdata = get_page(channel_url, quiet)
        # Just let the json library parse it.
        if strdata == None:
            log("Skip channle=%s, day=%d\n" % (channel, offset))
            continue
        total = json.loads(strdata)

        # and find relevant programming info
        try:
            v = list(total.values())[0]
            if isinstance(v, dict):
                v=list(v.values())
            elif not isinstance(v, (list,tuple)):
                raise TypeError
        except TypeError, LookupError:
            log("Unsubscriptable content (%s) from channel url: %r\n" % \
                    (v.values().__class__.__name__, channel_url))
            continue
        for r in v:
            if not isinstance(r, dict):
                log("Unparsable program content from channel id %s: %r\n" % (channel, r))
                continue
            
            # r is a dict, like:
            # {
            #  u'artikel_id': None,
            #  u'datum_end': u'2012-03-12 03:05:00',
            #  u'datum_start': u'2012-03-12 01:20:00',
            #  u'db_id': u'12379780',
            #  u'genre': u'Film',
            #  u'kijkwijzer': u'',
            #  u'soort': u'Zwarte komedie',
            #  u'titel': u'Der unauff\xe4llige Mr. Crane'
            # }
            
            program_url  = 'http://www.tvgids.nl/programma/' + r['db_id'] + '/'
            tdict = {}
            tdict['name']  = unescape(r['titel'])
            if tdict['name'] == '':
                log('Can not determine program title for "%s"' % program_url)
                continue
            tdict['start-time'] = match_to_date(retime.match(r['datum_start']),"begin time", tdict['name'])
            tdict['stop-time']  = match_to_date(retime.match(r['datum_end']), "eindtijd", tdict['name'])
            if tdict['start-time'] == None or tdict['stop-time'] == None:
                continue
            tdict['url']   = program_url
            tdict['ID']    = unescape(r['db_id'])
            tdict['offset'] = offset
            tdict['genre'] = unescape(r['genre']) if ('genre' in r and r['genre'] != None) else ''
            tdict['subgenre'] = unescape(r['soort']) if ('soort' in r and r['soort'] != None) else ''
            # and append the program to the list of programs
            
            programs.append(tdict)
    # done
    return programs

def parse_programs(programs, offset=0, quiet=0):
    """
    Parse a list of programs as generated by get_channel_all_days()  and
    adjust begin and end times to avoid gaps and overlap.
    """

    # good programs
    good_programs = []

    # sort all programs by startdate, enddate
    programs.sort(key=lambda program: (program['start-time'],program['stop-time']))
    
    # next, correct for missing end time and copy over all good programming to the 
    # good_programs list
    
    for i in range(len(programs)):

        # Try to correct missing end time by taking start time from next program on schedule
        if (programs[i]['stop-time'] == None and i < len(programs)-1):
            log('Oops, "%s" has no end time. Trying to fix...\n' % programs[i]['name'], quiet)
            programs[i]['stop-time'] = programs[i+1]['start-time']

        # The common case: start and end times are present and are not
        # equal to each other (yes, this can happen)
        if programs[i]['start-time'] != None and \
           programs[i]['stop-time']  != None and \
           programs[i]['start-time'] != programs[i]['stop-time']:
            good_programs.append(programs[i])

    # Han Holl: try to exclude programs that stop before they begin
    for i in range(len(good_programs)-1,-1,-1):
        if good_programs[i]['stop-time'] <= good_programs[i]['start-time']:
            log('Deleting invalid stop/start time: %s\n' % good_programs[i]['name'], quiet)
            del good_programs[i]

    # Try to exclude programs that only identify a group or broadcaster and have overlapping start/end times with
    # the actual programs
    for i in range(len(good_programs)-2,-1,-1):
          
        if good_programs[i]['start-time'] == good_programs[i+1]['start-time'] and \
           good_programs[i]['stop-time']  == good_programs[i+1]['stop-time'] and \
           good_programs[i]['name']  == good_programs[i+1]['name']:
            log('Deleting duplicate: %s\n' % good_programs[i]['name'], quiet)
            del good_programs[i]
        
        if good_programs[i]['start-time'] <= good_programs[i+1]['start-time'] and \
           good_programs[i]['stop-time']  >= good_programs[i+1]['stop-time']:
            log('Deleting grouping/broadcaster: %s\n' % good_programs[i]['name'], quiet)
            del good_programs[i]

    for i in range(len(good_programs)-1):

        # PdB: Fix tvgids start-before-end x minute interval overlap.  An overlap (positive or
        # negative) is halved and each half is assigned to the adjacent programmes. The maximum
        # overlap length between programming is set by the global variable 'max_overlap' and is 
        # default 10 minutes. Examples:
        #
        # Positive overlap (= overlap in programming):
        #   10:55 - 12:00 Lala
        #   11:55 - 12:20 Wawa
        # is transformed in:
        #   10:55 - 11.57 Lala
        #   11:57 - 12:20 Wawa
        # 
        # Negative overlap (= gap in programming):
        #   10:55 - 11:50 Lala
        #   12:00 - 12:20 Wawa
        # is transformed in:
        #   10:55 - 11.55 Lala
        #   11:55 - 12:20 Wawa
         
        stop  = good_programs[i]['stop-time']
        start = good_programs[i+1]['start-time']
        dt    = stop-start
        avg   = start + dt // 2
        overlap = 24*60*60*dt.days + dt.seconds

        # check for the size of the overlap
        if 0 < abs(overlap) <= max_overlap*60:
            if overlap > 0:
                log('"%s" and "%s" overlap %s minutes. Adjusting times.\n' % \
                    (good_programs[i]['name'],good_programs[i+1]['name'],overlap // 60), quiet)
            else:
                log('"%s" and "%s" have gap of %s minutes. Adjusting times.\n' % \
                    (good_programs[i]['name'],good_programs[i+1]['name'],abs(overlap) // 60), quiet)

            # stop-time of previous program wins
            if overlap_strategy == 'stop':
               good_programs[i+1]['start-time'] = good_programs[i]['stop-time']
            # start-time of next program wins
            elif overlap_strategy == 'start':
               good_programs[i]['stop-time'] = good_programs[i+1]['start-time']
            # average the difference
            elif overlap_strategy == 'average':
               good_programs[i]['stop-time']    = avg
               good_programs[i+1]['start-time'] = avg
            # leave as is
            else:
               pass

    # Experimental strategy to make sure programming does not disappear. All programs that overlap more
    # than the maximum overlap length, but less than the shortest length of the two programs are 
    # clumped.
    if do_clump:
        for i in range(len(good_programs)-1):
         
            stop  = good_programs[i]['stop-time']
            start = good_programs[i+1]['start-time']
            dt    = stop-start
            overlap = 24*60*60*dt.days + dt.seconds

            length0 = good_programs[i]['stop-time']   - good_programs[i]['start-time']
            length1 = good_programs[i+1]['stop-time'] - good_programs[i+1]['start-time']

            l0 = length0.days*24*60*60 + length0.seconds    
            l1 = length1.days*24*60*60 + length0.seconds    

            if abs(overlap) >= max_overlap*60 <= min(l0,l1)*60 and \
                'clumpidx' not in good_programs[i]   and \
                'clumpidx' not in good_programs[i+1]:
                good_programs[i]['clumpidx']   = '0/2'
                good_programs[i+1]['clumpidx'] = '1/2'
                good_programs[i]['stop-time'] = good_programs[i+1]['stop-time']
                good_programs[i+1]['start-time'] = good_programs[i]['start-time']
            

    # done, nothing to see here, please move on 
    return good_programs

def get_descriptions(programs, program_cache=None, nocattrans=0, quiet=0, slowdays=0):
    """
    Given a list of programs, from get_channel, retrieve program information
    """

    # This regexp tries to find details such as Genre, Acteurs, Jaar van Premiere etc.
    detail      = re.compile('<li>[^<]*<strong>([\w\- ]+):</strong>(.*?)</li>', re.DOTALL)

    # These regexps find the main description area and lines of descriptive text in this area
    description = re.compile('<div id="prog-content">(.*?)</div>',re.DOTALL)
    descrline = re.compile('<p>(.*?)</p>',re.DOTALL)

    # These regexps try to find the subgenre of the program, e.g. Fantasy-familiefilm, Comedyserie, 
    # Woonprogramma, Culinair Programma etc.
    # descrtype searches for the subgenre in the description area, e.g.:
    #      <strong>Woonprogramma</strong><p>Nance, Tooske, Ellemieke, Marlayne en Viktor helpen mensen...
    #
    # addprogtype searches for the subgenre in the special "mijn TV agenda" link, e.g.:
    #      <a href="#perstvgids" title="Plaats dit programma in mijn TV Agenda" 
    #                  onclick="addProg('10281755','Informatief','Woonprogramma.',event);return false;">

    descrtype = re.compile('<strong>([^<]*)</strong>',re.DOTALL)
    addprogtype = re.compile("addProg\(.*?,.*?,'(.*?)',.*?\)",re.DOTALL)

    # randomize detail requests
    nprograms = len(programs)
    fetch_order = list(range(0,nprograms))
    random.shuffle(fetch_order)

    counter = 0
    for i in fetch_order:
        counter += 1
        if programs[i]['offset'] >= slowdays:
            continue
        
        log('\n(%3.0f%%) %s: %s ' % (100*float(counter)/float(nprograms), i, programs[i]['name']), quiet)

        # check the cache for this program's ID
        cached_program = program_cache.query(programs[i]['ID'])
        
        if (cached_program != None):
            log(' [cached]', quiet)
            # copy the cached information, except the start/end times, rating and clumping, 
            # these may have changed.
            tstart = programs[i]['start-time']
            tstop  = programs[i]['stop-time']
            try:
                clump  = programs[i]['clumpidx']
            except LookupError:
                clump = False
            programs[i] = cached_program
            programs[i]['start-time'] = tstart
            programs[i]['stop-time']  = tstop
            if clump:
                programs[i]['clumpidx'] = clump
            continue

        # be nice to tvgids.nl
        time.sleep(random.randint(nice_time[0], nice_time[1]))

        # get the details page, and get all the detail nodes
        descriptions = ()
        details = ()
        try:
            log(' [normal fetch]', quiet)
            total = get_page(programs[i]['url'])
            details = detail.finditer(total)
            
            descrspan = description.search(total)
            if descrspan != None:
                descriptions = descrline.finditer(descrspan.group(1))
            else:
                log('Can not find program details on page\n', quiet)
                descriptions = []
            
        except Exception as e:
            # if we cannot find the description page, 
            # go to next in the loop
            log(' [fetch failed or timed out]', quiet)
            continue
        # define containers
        programs[i]['credits'] = {}
        programs[i]['video']   = {}

        # now parse the details
        programs[i]['details'] = []
        # First, we try to find the program type in the special "mijn TV Agenda" link, if not found there we
        # search for a type in the description section.
        # Note that this type is not the same as the generic genres (these are searched later on), 
        # but a more descriptive one like "Culinair programma" 
        # 
        def add_details(program, details):
            details = filter_line(details)
            if len(details) == 0:
                return
            if len(program['details']) > 0 and program['details'][-1].lower() == details.lower():
                return
            program['details'].append(details)
        
        if 'subgenre' in programs[i]:
            add_details(programs[i], programs[i]['subgenre'])
        
        m = addprogtype.search(total)
        if m:
            add_details(programs[i], m.group(1).capitalize())
        
        m = descrtype.search(descrspan.group(1))
        if m:
            add_details(programs[i], m.group(1).capitalize())

        # Secondly, we add one or more lines of the program description that are present.
        for descript in descriptions:
            # descript is a re.Match object
            descr_html = descript.group(1)
            
            # Remove sponsored link from description if present.
            sponsor_pos = descr_html.rfind('<i>Gesponsorde link:</i>')
            if sponsor_pos > 0:
                descr_html = descr_html[0:sponsor_pos]
            if re.search('[Gg]een detailgegevens be(?:kend|schikbaar)', descr_html):
                descr_html = ''
            
            add_details(programs[i], descr_html)
        
        if len(programs[i]['details']) == 0:
            programs[i]['detail1'] = ''
        else:
            programs[i]['detail1'] = programs[i]['details'][0]
        
        # Finally, we check out all program details. These are generically denoted as:
        #
        #   <li><strong>(TYPE):</strong>(CONTENT)</li> 
        #
        # Some examples:
        #
        #   <li><strong>Datum:</strong>16 oktober 2008</li>
        #   <li><strong>Genre:</strong>Amusement</li>
                                                                            
        for d in details:
            ctype = d.group(1).strip().lower()
            content_asis = filter_line(d.group(2))
            content = filter_line(content_asis)
            
            if content == '':
                continue

            elif ctype == 'genre':

                # Fix detection of movies based on description as tvgids.nl sometimes 
                # categorises a movie as e.g. "Komedie", "Misdaadkomedie", "Detectivefilm". 
                genre = filter_line(content.title())  # Titlecase
                if nocattrans:
                    programs[i]['genre'] = genre
                elif (programs[i]['detail1'].lower().find('film') != -1 \
                        or  programs[i]['detail1'].lower().find('komedie') != -1)\
                        and programs[i]['detail1'].lower().find('tekenfilm') == -1 \
                        and programs[i]['detail1'].lower().find('animatiekomedie') == -1 \
                        and programs[i]['detail1'].lower().find('filmpje') == -1:
                    programs[i]['genre'] = 'Film'
                else:
                    try:
                        programs[i]['genre'] = cattrans[genre.lower()].title()
                    except LookupError:
                        programs[i]['genre'] = genre

            # Parse persons and their roles for credit info
            elif ctype in roletrans:
                programs[i]['credits'][roletrans[ctype]] = []

                persons = content_asis.split(',');

                for name in persons:
                    if name.find(':') != -1:
                        name = name.split(':')[1]
                    if name.find('-') != -1:
                        name = name.split('-')[0]
                    if name.find('e.a') != -1:
                        name = name.split('e.a')[0]
                    programs[i]['credits'][roletrans[ctype]].append(filter_line(name))

            elif ctype == 'bijzonderheden':
                if content.find('Breedbeeld') != -1:
                    programs[i]['video']['breedbeeld'] = 1
                if content.find('Zwart') != -1: 
                    programs[i]['video']['blackwhite'] = 1
                if content.find('Teletekst') != -1: 
                    programs[i]['teletekst'] = 1
                if content.find('Stereo') != -1: 
                    programs[i]['stereo'] = 1
            elif ctype == 'url':
                programs[i]['infourl'] = filter_line(content)
            elif ctype not in programs[i]:
                # In unmatched cases, we still add the parsed type and content to the program details.
                # Some of these will lead to xmltv output during the xmlefy_programs step
                programs[i][filter_line(ctype)] = filter_line(content)

        # do not cache programming that is unknown at the time
        # of fetching.
        
        if programs[i]['name'].lower() != 'onbekend':
            program_cache.add(programs[i])

    log('\ndone...\n\n', quiet)
    
    # done
      
def title_split(program):
    """
    Some channels have the annoying habit of adding the subtitle to the title of a program. 
    This function attempts to fix this, by splitting the name at a ': '.
    """

    # Some programs (BBC3 when this happened) have no genre. If none, then set to a default
    if program['genre'] is None:
        program['genre'] = 'overige';
    
    if  ('titel aflevering' in program and program['titel aflevering'] not in (None, ''))  \
     or ('genre' in program and program['genre'].lower() in ['movies','film']):
       return

    colonpos =  program['name'].rfind(': ') 
    if colonpos > 0:
       program['titel aflevering'] = program['name'][colonpos+1:len(program['name'])].strip()
       program['name'] =  program['name'][0:colonpos].strip()

def xmlefy_programs(programs, channel, desc_len, compat=0, nocattrans=0, use_utc=0):
    """
    Given a list of programming (from get_channels())
    returns a unicode string with the xml equivalent
    We assume, that programs is in unicode.
    """
    output = []
    for program in programs:

        clumpidx = ''
        try:
            if 'clumpidx' in program:
                clumpidx = 'clumpidx="'+program['clumpidx']+'"'
        except LookupError:
            clumpidx = ''

        output.append('  <programme start="%s" stop="%s" channel="%s%s" %s> \n' % \
            (format_timezone(program['start-time'], use_utc), format_timezone(program['stop-time'], use_utc),\
             channel, compat and '.tvgids.nl' or '', clumpidx))

        output.append('    <title lang="nl">%s</title>\n' % xmlescape(program['name']))

        if 'titel aflevering' in program and program['titel aflevering'] != '':
            output.append('    <sub-title lang="nl">%s</sub-title>\n' % xmlescape(program['titel aflevering']))

        if 'details' in program:
            desc = program['details']
        elif 'detail2' in program:
            desc = [program[d] for d in ('detail1', 'detail2', 'detail3') if d in program]
        else:
            desc = []
        if desc != []:
            # join at most 4 lines of descriptions
            desc_line = ' '.join(desc[:3]).strip()
            if len(desc_line) > desc_len: 
                spacepos = desc_line[0:desc_len-3].rfind(' ') 
                desc_line = desc_line[0:spacepos] + '...'
            output.append('    <desc lang="nl">%s</desc>\n' % xmlescape(desc_line))
        
        # Process credits section if present.
        # This will generate director/actor/presenter info.
        if 'credits' in program and program['credits'] != {}:
            output.append('    <credits>\n')
            for role in program['credits']:
                for name in program['credits'][role]:
                    if name != '':
                        output.append('       <%s>%s</%s>\n' % (xmlescape(role), xmlescape(name), xmlescape(role)))
            output.append('    </credits>\n')

        if 'jaar van premiere' in program and program['jaar van premiere'] != '':
            output.append('    <date>%s</date>\n' % program['jaar van premiere'])

        if 'genre' in program and program['genre'] != '':
            output.append('    <category')
            if nocattrans:
               output.append(' lang="nl"')
            output.append ('>%s</category>\n' % xmlescape(program['genre']))
        
        if 'infourl' in program and program['infourl'] != '':
            output.append('    <url>%s</url>\n' % xmlescape(program['infourl'])) 

        if 'aflevering' in program and program['aflevering'] != '':
            output.append('    <episode-num system="onscreen">%s</episode-num>\n' % xmlescape(program['aflevering']))

        # Process video section if present
        if 'video' in program and program['video'] != {}:
            output.append('    <video>\n');
            if 'breedbeeld' in program['video']:
                output.append('      <aspect>16:9</aspect>\n')
            if 'blackwhite' in program['video']:
                output.append('      <colour>no</colour>\n')
            output.append('    </video>\n')

        if 'stereo' in program:
            output.append('    <audio><stereo>stereo</stereo></audio>\n')
 
        if 'teletekst' in program:
            output.append('    <subtitles type="teletext" />\n')

        # Set star-rating if applicable
        #if program['star-rating'] != '':
        #     output.append('    <star-rating><value>%s</value></star-rating>\n' % program['star-rating'])
                
        output.append('  </programme>\n')
    return output


def main():

    # Parse command line options
    try:
        opts, args = getopt.getopt(sys.argv[1:], "h", ["help", "output=", "capabilities", 
                                                       "preferredmethod", "days=", "offset=",
                                                       "configure", "fast", "slow",
                                                       "cache=", "clean_cache", "utc",
                                                       "slowdays=","compat",
                                                       "desc-length=","description","version",
                                                       "nocattrans","config-file=",
                                                       "max_overlap=", "overlap_strategy=",
                                                       "clear_cache", "quiet","logos="])
    except getopt.GetoptError:
        usage()
        return(2)

    # DEFAULT OPTIONS - Edit if you know what you are doing

    # where the output goes
    output      = None
    output_file = None

    # the total number of days to fetch 
    days        = 4

    # the first day to fetch (0=today, 1=tomorrow, etc)
    offset      = 0

    # Fetch data in fast mode, i.e. do NOT grab all the detail information,
    # fast means fast, because as it then does not have to fetch a web page for each program
    # Default: fast=0
    fast        = 0

    # number of days to fetch in slow mode. For example: --days 5 --slowdays 2, will 
    # fetch the first two days in slow mode (with all the details) and the remaining three
    # days in fast mode.
    slowdays    = 4

    # no output 
    quiet       = 0

    # insert url of channel logo into the xml data, this will be picked up by mythfilldatabase
    logos       = 1

    # enable this option if you were using tv_grab_nl, it adjusts the generated
    # xmltvid's so that everything works.
    compat      = 0
    
    # enable this option if you do not want the tvgids categories being translated into
    # MythTV-categories (genres)
    nocattrans  = 0

    # Maximum number of characters to use for program description.
    # Different values may work better in different versions of MythTV.
    desc_len = 475
 
    # default configuration file locations
    hpath = ''
    if 'HOME' in os.environ:
        hpath = os.environ['HOME']
    # extra test for windows users
    elif 'HOMEPATH' in os.environ:
        hpath = os.environ['HOMEPATH']

    # check Python version
    if sys.version_info[:2] < (2,6):
        log("tv_grab_nl_py requires Python 2.6 or higher\n")
        return(2)
    elif sys.version_info[:2] >= (3,0):
        log("tv_grab_nl_py does not yet support Python 3 or higher.\nExpect errors while we proceed\n")

    # hpath = ''
    xmltv_dir   = hpath+'/.xmltv'

    program_cache_file = xmltv_dir+'/program_cache'
    config_file = xmltv_dir+'/tv_grab_nl_py.conf'

    # cache the detail information. 
    program_cache = None
    clean_cache = 1
    clear_cache = 0

    # don't convert all the program date/times to UTC (GMT) timezone.
    # by default the current timezone is Europe/Amsterdam. This works fine
    # if you are located in the Amsterdam timezone, but not if you live abroad
    # in another timezone. If you want to use the UTC timestamp in combination
    # with mythtv, be sure to set the timezone in mythtv to 'auto'
    # (TimeOffset in Settings table)
    use_utc = False

    # seed the random generator
    random.seed(time.time())

    for o, a in opts:
        if o in ("-h", "--help"):
            usage()
            return(1)

        if o == "--quiet":
            quiet = 1;

        if o == "--description" or o == "--version":
            print("The Netherlands (tv_grab_nl_py version %s)" % VERSION)
            return(0)

        if o == "--capabilities":
            print("baseline")
            print("cache")
            print("manualconfig")
            print("preferredmethod")
            return(0)

        if o == '--preferredmethod':
            print('allatonce')
            return(0)

        if o == '--desc-length':
            # Use the requested length for programme descriptions.
            desc_len = int(a)
            log('Using description length: %d\n' % desc_len, quiet)

    for o, a in opts:
        if o == "--config-file":
            # use the provided name for configuration
            config_file = a
            log('Using config file: %s\n' % config_file, quiet)

    for o, a in opts:
        if o == "--configure":
            # check for the ~.xmltv dir
            config_dir = os.path.dirname(config_file)
            if (config_dir != '') and not os.path.exists(config_dir):
                log('Creating %s directory,' % config_dir, quiet)
                os.mkdir(config_dir)
            log('Creating config file: %s\n' % config_file, quiet)
            return(get_channels(config_file))

        if o == "--offset":
            # limit offset to maximum supported by tvgids.nl
            # Note: check later that days+offset<=6
            offset = min(int(a),3)

        if o == "--days":
            # limit days to maximum supported by tvgids.nl
            a = int(a)
            if a+offset > 4:
                log("tvgids.nl kan maximaal 3 dagen vooruit kijken.")
            days = min(a,4-offset)

        if o == "--slowdays":
            # limit slowdays to maximum supported by tvgids.nl
            slowdays = min(int(a),days)
            # slowdays implies fast == 0
            fast = 0

        if o == "--compat":
            compat = 1

        if o == "--nocattrans":
            nocattrans = 1

        if o == "--fast":
            fast = 1

        if o == "--output":
            output_file = a
            try:
                output = open(output_file,'w')
                sys.stdout = output
            except Exception:
                log('Cannot write to outputfile: %s\n' % output_file, quiet)
                return(2)

        if o == "--logos":
            logos = int(a)

        if o == "--clean_cache":
            clean_cache = 1
        if o == "--clear_cache":
            clear_cache = 1
        if o == "--cache":
            program_cache_file = a
        if o == "--max_overlap":
            max_overlap = int(a)
        if o == "--overlap_strategy":
            overlap_strategy = a
        if o == "--utc":
            use_utc = True

    # get configfile if available
    try:
        f = open(config_file,'rb')
    except IOError as e:
        if e.errno == 2:
            log('Config file %s not found.\n' % config_file)
            log('Re-run me with the --configure flag.\n')
        else:
            log('Config file %s: %s.\n' % (config_file, e.strerror))
        return(1)

    #check for cache
    program_cache = ProgramCache(program_cache_file)
    if clean_cache != 0:
        program_cache.clean()
    if clear_cache != 0:
        program_cache.clear()

    # Go!
    channels = {}

    # Read the channel stuff
    configencoding = 'iso-8859-1' # default encoding
    reconfigline = re.compile(r'#\s*(\w+):\s*(.+)')
    for byteline in f.readlines():
        try:
            line = byteline.decode(configencoding)
            line = line.lstrip()
            line = line.replace('\n','')
        except UnicodeError:
            log('Config file %s is not encoded in %s.\n' % (config_file, configencoding))
            return(1)
        if len(line) > 0 and line[0] == '#':
            match = reconfigline.match(line)
            if match is not None and match.group(1) == "encoding":
                configencoding = match.group(2)
                try:
                    codecs.getencoder(configencoding)
                except LookupError:
                    log('Config file %s has invalid encoding %s.\n' % (config_file, configencoding))
                    return(1)
            continue
        else:
            try:
                channel = line.split(None, 1) # split on first whitespace
                channels[int(channel[0])] = channel[1]
            except Exception:
                log('Invalid line in config file %s: %r\n' % (config_file, line))
    
    try:
        f.close()
    except IOError:
        pass

    # channels are now in channels dict keyed on channel id

    # print header stuff
    xmlencoding = 'UTF-8'
    xml = []
    xml.append('<?xml version="1.0" encoding="%s"?>\n' % xmlencoding)
    xml.append('<!DOCTYPE tv SYSTEM "xmltv.dtd">\n')
    xml.append('<tv generator-info-name="tv_grab_nl_py (version %s)">\n' % VERSION)

    # first do the channel info
    for key in channels.keys():
        xml.append('  <channel id="%s%s">\n' % (key, compat and '.tvgids.nl' or ''))
        xml.append('    <display-name lang="nl">%s</display-name>\n' % xmlescape(channels[key]))
        if (logos):
            try:
                ikey = int(key)
                if ikey in logo_names:
                    full_logo_url = logo_provider[logo_names[ikey][0]]+logo_names[ikey][1]+'.gif'
                    xml.append('    <icon src="%s" />\n' % full_logo_url)
            except Exception:
                pass
        xml.append('  </channel>\n')

    num_chans = len(channels.keys())
    channel_cnt = 0
    if program_cache != None:
        program_cache.clean()

    fluffy = channels.keys()
    nfluffy = len(fluffy)
    for id in fluffy:
        channel_cnt += 1
        log('\n\nNow fetching %s(xmltvid=%s%s) (channel %s of %s)\n' % \
                (channels[id], id, (compat and '.tvgids.nl' or ''), channel_cnt, nfluffy), quiet)
        info = get_channel_all_days(id, offset, days, quiet)
        programs = parse_programs(info, None, quiet)

        # fetch descriptions
        if not fast:
           get_descriptions(programs, program_cache, nocattrans, quiet, slowdays)
        
        # Split titles with colon in it
        # Note: this only takes place if all days retrieved are also grabbed with details (slowdays=days)
        # otherwise this function might change some titles after a few grabs and thus may result in
        # loss of programmed recordings for these programs.
        if slowdays == days:
            for program in programs:
               title_split(program)

        xml.extend(xmlefy_programs(programs, id, desc_len, compat, nocattrans, use_utc))

        # save the cache after each channel fetch 
        if program_cache != None:
            program_cache.dump(program_cache_file)
    
        # be nice to tvgids.nl
        
        time.sleep(random.randint(nice_time[0], nice_time[1]))
        if program_cache != None:
            program_cache.dump(program_cache_file)

    # print footer stuff
    xml.append('</tv>')

    # print result to stdout
    xml = "".join(xml)
    print(xml.encode('utf-8'))

    # close the outputfile if necessary
    if output != None:
        output.close()

    # and return success
    return(0)

# allow this to be a module
if __name__ == '__main__':
    sys.exit(main())


